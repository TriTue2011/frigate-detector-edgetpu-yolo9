{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# YOLOv9-s ReLU6 Training for EdgeTPU\n",
        "\n",
        "This notebook fine-tunes YOLOv9-s with ReLU6 activations for better INT8 quantization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpu_check"
      },
      "source": [
        "## Step 1: Check GPU and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup",
        "outputId": "f6d8f7fc-20d8-425e-bb9d-d4e69f343e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "SETUP: Checking GPU\n",
            "======================================================================\n",
            "âœ“ GPU detected: Tesla T4\n",
            "  Memory: 15.83 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SETUP: Checking GPU\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ“ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸  NO GPU DETECTED!\")\n",
        "    print(\"   Go to: Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
        "    print(\"   Then restart this notebook\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clone_repo"
      },
      "source": [
        "## Step 2: Clone YOLOv9 Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clone",
        "outputId": "fe1a758b-51e8-4707-8623-ca6e16fd3b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Cloning YOLOv9 Repository\n",
            "======================================================================\n",
            "Cloning into 'yolov9'...\n",
            "remote: Enumerating objects: 781, done.\u001b[K\n",
            "remote: Total 781 (delta 0), reused 0 (delta 0), pack-reused 781 (from 1)\u001b[K\n",
            "Receiving objects: 100% (781/781), 3.27 MiB | 4.74 MiB/s, done.\n",
            "Resolving deltas: 100% (330/330), done.\n",
            "âœ“ Repository cloned\n",
            "âœ“ Working directory: /content/yolov9\n",
            "\n",
            "Installing requirements...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ“ Requirements installed\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"Cloning YOLOv9 Repository\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if not Path('yolov9').exists():\n",
        "    !git clone https://github.com/WongKinYiu/yolov9.git\n",
        "    print(\"âœ“ Repository cloned\")\n",
        "else:\n",
        "    print(\"âœ“ Repository already exists\")\n",
        "\n",
        "# Navigate to repo\n",
        "os.chdir('/content/yolov9')\n",
        "print(f\"âœ“ Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Install requirements\n",
        "print(\"\\nInstalling requirements...\")\n",
        "!pip install -q -r requirements.txt\n",
        "print(\"âœ“ Requirements installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_relu"
      },
      "source": [
        "## Step 5: Create ReLU6 Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHzMYpBm9Ide"
      },
      "outputs": [],
      "source": [
        "# needed for model definition yaml\n",
        "# keep only the classes that are relevant for object detection in surveillance\n",
        "CLASSES_TO_KEEP = [\n",
        "    0,   # person\n",
        "    1,   # bicycle\n",
        "    2,   # car\n",
        "    3,   # motorcycle\n",
        "    4,   # airplane\n",
        "    5,   # bus\n",
        "    6,   # train\n",
        "    7,   # truck\n",
        "    8,   # boat\n",
        "    14,  # bird\n",
        "    15,  # cat\n",
        "    16,  # dog\n",
        "    17,  # horse\n",
        "    18,  # sheep\n",
        "    19,  # cow\n",
        "    20,  # elephant\n",
        "    21,  # bear\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "create_relu_config",
        "outputId": "08af4f61-a8a2-4b19-dd88-88fe36a8cd6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Creating ReLU6 Model Configuration with DualDDetectEdgeTPU\n",
            "======================================================================\n",
            "âœ“ Changed number of classes to 17\n",
            "âœ“ Set activation: nn.ReLU6()\n",
            "âœ“ Changed detection head: DualDDetect â†’ DualDDetectEdgeTPU\n",
            "âœ“ Created: models/detect/yolov9-s-relu6.yaml\n",
            "\n",
            "Verifying configuration:\n",
            "  Activation: activation: nn.ReLU6()\n",
            "  Detection head: [[28, 25, 22, 15, 18, 21], 1, DualDDetectEdgeTPU, [nc]],  # Detect(P3, P4, P5)\n",
            "\n",
            "âœ… Configuration ready for EdgeTPU quantization:\n",
            "   - ReLU6 activation (bounded [0,6] for all layers)\n",
            "   - DualDDetectTwoOutputs (separate box/class tensors)\n",
            "   - Both prevent quantization value crushing\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"Creating ReLU6 Model Configuration with DualDDetectEdgeTPU\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Read the original config\n",
        "config_path = Path('models/detect/yolov9-s.yaml')\n",
        "with open(config_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Create ReLU6 + DualDDetectTwoOutputs version\n",
        "relu6_config_path = Path('models/detect/yolov9-s-relu6.yaml')\n",
        "new_lines = []\n",
        "\n",
        "for line in lines:\n",
        "    if line.strip() == '#activation: nn.ReLU()':\n",
        "        # Uncomment and change to ReLU6\n",
        "        new_lines.append('activation: nn.ReLU6()\\n')\n",
        "        print(\"âœ“ Set activation: nn.ReLU6()\")\n",
        "    elif line.strip().startswith('#activation:'):\n",
        "        # Keep other activation lines commented\n",
        "        new_lines.append(line)\n",
        "    elif 'DualDDetect' in line and 'DualDDetectEdgeTPU' not in line:\n",
        "        # Replace DualDDetect with DualDDetectEdgeTPU\n",
        "        new_line = line.replace('DualDDetect', 'DualDDetectEdgeTPU')\n",
        "        new_lines.append(new_line)\n",
        "        print(f\"âœ“ Changed detection head: DualDDetect â†’ DualDDetectEdgeTPU\")\n",
        "    elif line.strip() == 'nc: 80  # number of classes':\n",
        "        new_lines.append(f'nc: {len(CLASSES_TO_KEEP)}  # number of classes\\n')\n",
        "        print(f\"âœ“ Changed number of classes to {len(CLASSES_TO_KEEP)}\")\n",
        "    else:\n",
        "        new_lines.append(line)\n",
        "\n",
        "# Save config\n",
        "with open(relu6_config_path, 'w') as f:\n",
        "    f.writelines(new_lines)\n",
        "print(f\"âœ“ Created: {relu6_config_path}\")\n",
        "\n",
        "# Verify the changes\n",
        "print(\"\\nVerifying configuration:\")\n",
        "with open(relu6_config_path, 'r') as f:\n",
        "    content = f.read()\n",
        "    for line in content.split('\\n'):\n",
        "        if 'activation:' in line and not line.strip().startswith('#'):\n",
        "            print(f\"  Activation: {line.strip()}\")\n",
        "        if 'DualDDetect' in line:\n",
        "            print(f\"  Detection head: {line.strip()}\")\n",
        "\n",
        "print(\"\\nâœ… Configuration ready for EdgeTPU quantization:\")\n",
        "print(\"   - ReLU6 activation (bounded [0,6] for all layers)\")\n",
        "print(\"   - DualDDetectTwoOutputs (separate box/class tensors)\")\n",
        "print(\"   - Both prevent quantization value crushing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWTEDxIvdRJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb81b7dc-9969-47a6-d393-ad43cc44eba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated 1 path from the index\n"
          ]
        }
      ],
      "source": [
        "# reset / clean up for use in re-runs\n",
        "\n",
        "if False:\n",
        "    os.chdir('/content/yolov9')\n",
        "    !rm -rf runs/train/yolov9-s-relu6\n",
        "\n",
        "if False:\n",
        "    !git checkout models/yolo.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxFHEtKf6q9i",
        "outputId": "f6412b0e-565e-4a61-cf76-c335c8e686bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "DualDDetectEdgeTPU - Two Tensors Output\n",
            "======================================================================\n",
            "======================================================================\n",
            "Adding DualDDetectEdgeTPU to models/yolo.py\n",
            "======================================================================\n",
            "âœ… Found insertion point at line 259 (before TripleDetect)\n",
            "âœ… Successfully added DualDDetectEdgeTPU class\n",
            "\n",
            "======================================================================\n",
            "Updating parse_model to recognize DualDDetectEdgeTPU\n",
            "======================================================================\n",
            "âœ… Updated: elif m in {Detect, DualDetect, TripleDetect, DDete...\n",
            "âœ… Updated: if isinstance(m, (Detect, DualDetect, TripleDetect...\n",
            "âœ… Updated: if isinstance(m, (DualDetect, TripleDetect, DualDD...\n",
            "\n",
            "âœ… All updates completed successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"DualDDetectEdgeTPU - Two Tensors Output\")\n",
        "print(\"=\"*70)\n",
        "# This version keeps the tensors separate to allow different ranges of values\n",
        "# that the EdgeTPU can quantize individually.\n",
        "\n",
        "import os\n",
        "os.chdir('/content/yolov9')\n",
        "\n",
        "dualddetect_edgetpu_code = '''\n",
        "class DualDDetectEdgeTPU(nn.Module):\n",
        "    \"\"\"\n",
        "    EdgeTPU-optimized DualDDetect that outputs two tensores with:\n",
        "    - DFL distribution for calculating box coordinates\n",
        "    - Logit value class scores clamped to [-4, 4], or about 2% to 98% probability\n",
        "\n",
        "    Both ranges are compatible for INT8 quantization.\n",
        "    \"\"\"\n",
        "\n",
        "    dynamic = False\n",
        "    export = False\n",
        "    shape = None\n",
        "    anchors = torch.empty(0)\n",
        "    strides = torch.empty(0)\n",
        "\n",
        "    def __init__(self, nc=80, ch=(), inplace=True):\n",
        "        super().__init__()\n",
        "        self.nc = nc\n",
        "        self.nl = len(ch) // 2\n",
        "        self.reg_max = 16\n",
        "        self.no = nc + self.reg_max * 4\n",
        "        self.inplace = inplace\n",
        "        self.stride = torch.zeros(self.nl)\n",
        "\n",
        "        c2 = make_divisible(max((ch[0] // 4, self.reg_max * 4, 16)), 4)\n",
        "        c3 = max((ch[0], min((self.nc * 2, 128))))\n",
        "        c4 = make_divisible(max((ch[self.nl] // 4, self.reg_max * 4, 16)), 4)\n",
        "        c5 = max((ch[self.nl], min((self.nc * 2, 128))))\n",
        "\n",
        "        # Branch 1 (auxiliary)\n",
        "        self.cv2 = nn.ModuleList(\n",
        "            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3, g=4),\n",
        "                         nn.Conv2d(c2, 4 * self.reg_max, 1, groups=4))\n",
        "            for x in ch[:self.nl]\n",
        "        )\n",
        "        self.cv3 = nn.ModuleList(\n",
        "            nn.Sequential(Conv(x, c3, 3), Conv(c3, c3, 3), nn.Conv2d(c3, self.nc, 1))\n",
        "            for x in ch[:self.nl]\n",
        "        )\n",
        "\n",
        "        # Branch 2 (main)\n",
        "        self.cv4 = nn.ModuleList(\n",
        "            nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3, g=4),\n",
        "                         nn.Conv2d(c4, 4 * self.reg_max, 1, groups=4))\n",
        "            for x in ch[self.nl:]\n",
        "        )\n",
        "        self.cv5 = nn.ModuleList(\n",
        "            nn.Sequential(Conv(x, c5, 3), Conv(c5, c5, 3), nn.Conv2d(c5, self.nc, 1))\n",
        "            for x in ch[self.nl:]\n",
        "        )\n",
        "\n",
        "        self.dfl = DFL(self.reg_max)\n",
        "        self.dfl2 = DFL(self.reg_max)\n",
        "\n",
        "    def forward(self, x):\n",
        "        shape = x[0].shape  # BCHW\n",
        "        d1 = []\n",
        "        d2 = []\n",
        "\n",
        "        for i in range(self.nl):\n",
        "            d1.append(torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1))\n",
        "            d2.append(torch.cat((self.cv4[i](x[self.nl+i]), self.cv5[i](x[self.nl+i])), 1))\n",
        "\n",
        "        if self.training:\n",
        "            return [d1, d2]\n",
        "\n",
        "        if self.export:\n",
        "            # Inference path: keep boxes and classes separate on edgetpu\n",
        "            # Output in non-standard B(H*W)C format instead of BC(H*W)\n",
        "            # to avoid limits of EdgeTPU concatenation sizes\n",
        "            # AND to facilitate post-processing on CPU\n",
        "\n",
        "            boxes_raw = torch.cat([\n",
        "                self.cv4[i](x[self.nl+i]).permute(0, 2, 3, 1).flatten(1, 2)\n",
        "                for i in range(self.nl)\n",
        "            ], dim=1)\n",
        "\n",
        "            classes_raw = torch.cat([\n",
        "                torch.clamp(self.cv5[i](x[self.nl+i]), -4.0, 4.0).permute(0, 2, 3, 1).flatten(1, 2)\n",
        "                for i in range(self.nl)\n",
        "            ], dim=1)\n",
        "\n",
        "            return boxes_raw, classes_raw\n",
        "        else:\n",
        "            # Validation path\n",
        "            # Set up anchors if needed\n",
        "            if self.dynamic or self.shape != shape:\n",
        "                self.anchors, self.strides = (d1.transpose(0, 1) for d1 in make_anchors(d1, self.stride, 0.5))\n",
        "                self.shape = shape\n",
        "\n",
        "            # Decode boxes\n",
        "            box, cls = torch.cat([di.view(shape[0], self.no, -1) for di in d1], 2).split((self.reg_max * 4, self.nc), 1)\n",
        "            dbox = dist2bbox(self.dfl(box), self.anchors.unsqueeze(0), xywh=True, dim=1) * self.strides\n",
        "\n",
        "            box2, cls2 = torch.cat([di.view(shape[0], self.no, -1) for di in d2], 2).split((self.reg_max * 4, self.nc), 1)\n",
        "            dbox2 = dist2bbox(self.dfl2(box2), self.anchors.unsqueeze(0), xywh=True, dim=1) * self.strides\n",
        "\n",
        "            # Apply sigmoid to classes\n",
        "            cls_sigmoid = cls.sigmoid()\n",
        "            cls2_sigmoid = cls2.sigmoid()\n",
        "            # Validation needs PIXEL coordinates\n",
        "            y_aux = torch.cat((dbox, cls_sigmoid), 1)\n",
        "            y_main = torch.cat((dbox2, cls2_sigmoid), 1)  # NO normalization!\n",
        "            return ([y_aux, y_main], [d1, d2])\n",
        "\n",
        "    def bias_init(self):\n",
        "        \"\"\"Initialize biases\"\"\"\n",
        "        m = self\n",
        "        for a, b, s in zip(m.cv2, m.cv3, m.stride):\n",
        "            a[-1].bias.data[:] = 1.0\n",
        "            b[-1].bias.data[:m.nc] = math.log(5 / m.nc / (640 / s) ** 2)\n",
        "        for a, b, s in zip(m.cv4, m.cv5, m.stride):\n",
        "            a[-1].bias.data[:] = 1.0\n",
        "            b[-1].bias.data[:m.nc] = math.log(5 / m.nc / (640 / s) ** 2)\n",
        "\n",
        "'''\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Adding DualDDetectEdgeTPU to models/yolo.py\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Read the file\n",
        "with open('models/yolo.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Check if code already present\n",
        "code_already_present = False\n",
        "for i, line in enumerate(lines):\n",
        "    if 'class DualDDetectEdgeTPU(nn.Module):' in line:\n",
        "        code_already_present = True\n",
        "        print(f\"âœ… Code already present at line {i+1}\")\n",
        "        break\n",
        "\n",
        "# Find where to insert (after DualDDetect class)\n",
        "insert_idx = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if 'class TripleDetect(nn.Module):' in line:\n",
        "        insert_idx = i\n",
        "        print(f\"âœ… Found insertion point at line {i+1} (before TripleDetect)\")\n",
        "        break\n",
        "\n",
        "if insert_idx == -1:\n",
        "    print(\"âŒ Could not find TripleDetect class\")\n",
        "    print(\"Looking for alternative insertion point...\")\n",
        "    for i, line in enumerate(lines):\n",
        "        if 'class DualDDetect(nn.Module):' in line:\n",
        "            # Find the end of DualDDetect\n",
        "            for j in range(i+1, len(lines)):\n",
        "                if lines[j].strip().startswith('class ') and j > i + 10:\n",
        "                    insert_idx = j\n",
        "                    print(f\"âœ… Found insertion point at line {j+1}\")\n",
        "                    break\n",
        "            break\n",
        "\n",
        "if insert_idx == -1:\n",
        "    print(\"âŒ Error: Could not find insertion point!\")\n",
        "elif code_already_present:\n",
        "    print(\"âœ… Code already present\")\n",
        "else:\n",
        "    # Insert the new class\n",
        "    lines.insert(insert_idx, '\\n\\n' + dualddetect_edgetpu_code + '\\n\\n')\n",
        "\n",
        "    # Write back\n",
        "    with open('models/yolo.py', 'w') as f:\n",
        "        f.writelines(lines)\n",
        "\n",
        "    print(\"âœ… Successfully added DualDDetectEdgeTPU class\")\n",
        "\n",
        "    # Now update parse_model to recognize the new class\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Updating parse_model to recognize DualDDetectEdgeTPU\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Read again\n",
        "    with open('models/yolo.py', 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Update all the isinstance checks\n",
        "    updates = [\n",
        "        ('elif m in {Detect, DualDetect, TripleDetect, DDetect, DualDDetect, TripleDDetect, Segment, DSegment, DualDSegment, Panoptic}:',\n",
        "         'elif m in {Detect, DualDetect, TripleDetect, DDetect, DualDDetect, DualDDetectEdgeTPU, TripleDDetect, Segment, DSegment, DualDSegment, Panoptic}:'),\n",
        "\n",
        "        ('if isinstance(m, (Detect, DualDetect, TripleDetect, DDetect, DualDDetect, TripleDDetect, Segment, DSegment, DualDSegment, Panoptic)):',\n",
        "         'if isinstance(m, (Detect, DualDetect, TripleDetect, DDetect, DualDDetect, DualDDetectEdgeTPU, TripleDDetect, Segment, DSegment, DualDSegment, Panoptic)):'),\n",
        "\n",
        "        ('if isinstance(m, (DualDetect, TripleDetect, DualDDetect, TripleDDetect, DualDSegment)):',\n",
        "         'if isinstance(m, (DualDetect, TripleDetect, DualDDetect, DualDDetectEdgeTPU, TripleDDetect, DualDSegment)):'),\n",
        "    ]\n",
        "\n",
        "    for old, new in updates:\n",
        "        if old in content:\n",
        "            content = content.replace(old, new)\n",
        "            print(f\"âœ… Updated: {old[:50]}...\")\n",
        "\n",
        "    # Write back\n",
        "    with open('models/yolo.py', 'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "    print(\"\\nâœ… All updates completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_weights"
      },
      "source": [
        "## Step 4: Download Pretrained Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weights_download",
        "outputId": "7eabb677-901f-4b6c-88a0-335c837baa0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Downloading YOLOv9-t Pretrained Weights (based on SiLU activation and 80 classes)\n",
            "======================================================================\n",
            "Downloading pretrained weights...\n",
            "yolov9-s-converted. 100%[===================>]  14.35M  76.0MB/s    in 0.2s    \n",
            "âœ“ Weights downloaded\n",
            "  File size: 15.04 MB\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"Downloading YOLOv9-s Pretrained Weights (based on SiLU activation and 80 classes)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "weights_file = Path('yolov9-s-converted.pt')\n",
        "\n",
        "if not weights_file.exists():\n",
        "    print(\"Downloading pretrained weights...\")\n",
        "    !wget -q --show-progress https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-s-converted.pt\n",
        "    print(\"âœ“ Weights downloaded\")\n",
        "else:\n",
        "    print(\"âœ“ Weights already exist\")\n",
        "\n",
        "print(f\"  File size: {weights_file.stat().st_size / 1e6:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_data"
      },
      "source": [
        "## Step 3: Download COCO Dataset\n",
        "\n",
        "**This takes 10-15 minutes and downloads ~20GB**\n",
        "\n",
        "Only needs to be done once - Colab will cache it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coco_download",
        "outputId": "a251dfd3-a4b2-4854-da26-5dd934517014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Downloading COCO Dataset (YOLO Format)\n",
            "======================================================================\n",
            "\n",
            "Downloading COCO dataset in YOLO format...\n",
            "This includes both images and labels in the correct structure\n",
            "coco2017labels.zip  100%[===================>]  46.39M  53.7MB/s    in 0.9s    \n",
            "train2017.zip       100%[===================>]  18.01G  43.9MB/s    in 7m 21s  \n",
            "val2017.zip         100%[===================>] 777.80M  45.3MB/s    in 18s     \n",
            "\n",
            "âœ“ COCO dataset downloaded with correct structure!\n",
            "\n",
            "Structure:\n",
            "  coco/\n",
            "    images/\n",
            "      train2017/\n",
            "      val2017/\n",
            "    labels/\n",
            "      train2017/\n",
            "      val2017/\n",
            "\n",
            "âœ“ Train: 118,287 images, 117,266 labels\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Download Pre-Converted COCO Dataset with YOLO Format Labels\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"Downloading COCO Dataset (YOLO Format)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Remove the old incorrect structure\n",
        "import shutil\n",
        "\n",
        "if False:\n",
        "    if Path('coco').exists():\n",
        "        shutil.rmtree('coco')\n",
        "        print(\"âœ“ Removed old COCO directory\")\n",
        "\n",
        "if not Path('coco').exists():\n",
        "\n",
        "    # Download COCO in YOLO format (images + labels already structured correctly)\n",
        "    print(\"\\nDownloading COCO dataset in YOLO format...\")\n",
        "    print(\"This includes both images and labels in the correct structure\")\n",
        "\n",
        "    TEST_SET = False # can reduce data size with this\n",
        "\n",
        "    !mkdir -p coco/images\n",
        "    if not Path('coco2017labels.zip').exists():\n",
        "        !wget -q --show-progress https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels.zip\n",
        "    !unzip -q coco2017labels.zip # -d coco\n",
        "    if TEST_SET:\n",
        "        if not Path('coco128.zip').exists():\n",
        "            !wget -q --show-progress https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip\n",
        "        !rm -rf coco/labels/train2017\n",
        "        !rm -rf coco/images/train2017\n",
        "        !unzip -q coco128.zip\n",
        "        !mv coco128/images/* coco/images/\n",
        "        !mv coco128/labels/* coco/labels/\n",
        "        !rm -rf coco128\n",
        "        !rm coco/train2017.txt\n",
        "        !ls coco/images/train2017/*.jpg | sed 's|^coco|.|' > coco/train2017.txt\n",
        "    else:\n",
        "        if not Path('train2017.zip').exists():\n",
        "            !wget -q --show-progress http://images.cocodataset.org/zips/train2017.zip\n",
        "        !unzip -q train2017.zip -d coco/images\n",
        "    if not Path('val2017.zip').exists():\n",
        "        !wget -q --show-progress http://images.cocodataset.org/zips/val2017.zip\n",
        "    !unzip -q -n val2017.zip -d coco/images\n",
        "\n",
        "    # Cleanup\n",
        "    #!rm coco2017labels.zip train2017.zip val2017.zip\n",
        "\n",
        "    print(\"\\nâœ“ COCO dataset downloaded with correct structure!\")\n",
        "    print(\"\\nStructure:\")\n",
        "    print(\"  coco/\")\n",
        "    print(\"    images/\")\n",
        "    print(\"      train2017/\")\n",
        "    print(\"      val2017/\")\n",
        "    print(\"    labels/\")\n",
        "    print(\"      train2017/\")\n",
        "    print(\"      val2017/\")\n",
        "\n",
        "# Verify\n",
        "train_imgs = len(list(Path('coco/images/train2017').glob('*.jpg')))\n",
        "train_lbls = len(list(Path('coco/labels/train2017').glob('*.txt')))\n",
        "print(f\"\\nâœ“ Train: {train_imgs:,} images, {train_lbls:,} labels\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cleanup commands in case of re-try\n",
        "\n",
        "#!rm -rf coco\n",
        "#!unzip -q coco2017labels.zip # -d coco\n",
        "#!unzip -q val2017.zip -d coco/images\n",
        "#!unzip -q -n val2017.zip -d coco/images"
      ],
      "metadata": {
        "id": "i0THhtR6fv1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM6zmAjCDtJT",
        "outputId": "1f6d1dbc-3ac4-4b2e-b30b-14ec36587cea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Filtering COCO Dataset to Your Classes\n",
            "======================================================================\n",
            "\n",
            "Keeping 17 classes:\n",
            "\n",
            "Filtering labels...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  train2017: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 117266/117266 [00:51<00:00, 2295.91it/s]\n",
            "  val2017: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:01<00:00, 2624.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Filtering complete:\n",
            "\n",
            "Training set:\n",
            "  Images: 118,287 â†’ 86,145 (72.8%)\n",
            "  Objects: 849,947 â†’ 404,191 (47.6%)\n",
            "\n",
            "Validation set:\n",
            "  Images: 5,000 â†’ 3,645 (72.9%)\n",
            "  Objects: 36,335 â†’ 16,998 (46.8%)\n",
            "\n",
            "âœ“ Labels replaced (original backed up to labels_full)\n",
            "âœ“ Cache cleared\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Filter COCO Dataset to 17 Specific Classes\n",
        "# ============================================================================\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Filtering COCO Dataset to Your Classes\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nKeeping {len(CLASSES_TO_KEEP)} classes:\")\n",
        "\n",
        "# Map old COCO IDs to new sequential IDs\n",
        "old_to_new = {old_id: new_id for new_id, old_id in enumerate(CLASSES_TO_KEEP)}\n",
        "\n",
        "def filter_labels(split='train2017'):\n",
        "    \"\"\"Filter labels to only include specified classes\"\"\"\n",
        "    label_dir = Path(f'coco/labels/{split}')\n",
        "    filtered_dir = Path(f'coco/labels_filtered/{split}')\n",
        "    filtered_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    kept_images = 0\n",
        "    total_objects_before = 0\n",
        "    total_objects_after = 0\n",
        "\n",
        "    for label_file in tqdm(list(label_dir.glob('*.txt')), desc=f\"  {split}\"):\n",
        "        with open(label_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        total_objects_before += len(lines)\n",
        "\n",
        "        # Filter and remap class IDs\n",
        "        filtered_lines = []\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 5:\n",
        "                class_id = int(parts[0])\n",
        "                if class_id in old_to_new:\n",
        "                    # Remap to new sequential ID\n",
        "                    new_class_id = old_to_new[class_id]\n",
        "                    filtered_lines.append(f\"{new_class_id} {' '.join(parts[1:])}\\n\")\n",
        "\n",
        "        # Only keep label files for images that have at least one relevant object\n",
        "        if filtered_lines:\n",
        "            output_file = filtered_dir / label_file.name\n",
        "            with open(output_file, 'w') as f:\n",
        "                f.writelines(filtered_lines)\n",
        "            kept_images += 1\n",
        "            total_objects_after += len(filtered_lines)\n",
        "\n",
        "    return kept_images, total_objects_before, total_objects_after\n",
        "\n",
        "# Filter both train and val\n",
        "print(\"\\nFiltering labels...\")\n",
        "train_kept, train_before, train_after = filter_labels('train2017')\n",
        "val_kept, val_before, val_after = filter_labels('val2017')\n",
        "\n",
        "print(f\"\\nâœ“ Filtering complete:\")\n",
        "print(f\"\\nTraining set:\")\n",
        "print(f\"  Images: 118,287 â†’ {train_kept:,} ({train_kept/118287*100:.1f}%)\")\n",
        "print(f\"  Objects: {train_before:,} â†’ {train_after:,} ({train_after/train_before*100:.1f}%)\")\n",
        "print(f\"\\nValidation set:\")\n",
        "print(f\"  Images: 5,000 â†’ {val_kept:,} ({val_kept/5000*100:.1f}%)\")\n",
        "print(f\"  Objects: {val_before:,} â†’ {val_after:,} ({val_after/val_before*100:.1f}%)\")\n",
        "\n",
        "# Backup and replace\n",
        "if Path('coco/labels_full').exists():\n",
        "    shutil.rmtree('coco/labels_full')\n",
        "shutil.move('coco/labels', 'coco/labels_full')\n",
        "shutil.move('coco/labels_filtered', 'coco/labels')\n",
        "print(f\"\\nâœ“ Labels replaced (original backed up to labels_full)\")\n",
        "\n",
        "# Delete old cache\n",
        "for cache in Path('coco/labels').rglob('*.cache'):\n",
        "    cache.unlink()\n",
        "print(f\"âœ“ Cache cleared\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN6KCyT3FYHL",
        "outputId": "75e46a4e-70df-42d0-bfde-c1647d1a892a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Subsampling to 10% of Filtered Dataset\n",
            "======================================================================\n",
            "\n",
            "âœ“ Subsampling complete:\n",
            "  Training: 86,145 â†’ 8,614 images\n",
            "  Validation: 3,645 â†’ 364 images\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Subsample Filtered Dataset\n",
        "# ============================================================================\n",
        "import random\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "SUBSAMPLE_PERCENT = 10  # Use 20% of data\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"Subsampling to {SUBSAMPLE_PERCENT}% of Filtered Dataset\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "SUBSAMPLE_SUFFIX = \"_sub\"\n",
        "\n",
        "def subsample_dataset(split='train2017', percent=15):\n",
        "    label_dir = Path(f'coco/labels/{split}')\n",
        "    img_dir = Path(f'coco/images/{split}')\n",
        "\n",
        "    # Get all labels\n",
        "    all_labels = list(label_dir.glob('*.txt'))\n",
        "\n",
        "    # Random subsample\n",
        "    random.seed(42)  # Reproducible\n",
        "    n_keep = int(len(all_labels) * percent / 100)\n",
        "    selected = random.sample(all_labels, n_keep)\n",
        "\n",
        "    # Create subsample directories\n",
        "    sub_label_dir = Path(f'coco/labels/{split}{SUBSAMPLE_SUFFIX}')\n",
        "    sub_img_dir = Path(f'coco/images/{split}{SUBSAMPLE_SUFFIX}')\n",
        "    sub_label_dir.mkdir(parents=True, exist_ok=True)\n",
        "    sub_img_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Copy selected files\n",
        "    for label_file in selected:\n",
        "        # Copy label\n",
        "        shutil.copy(label_file, sub_label_dir / label_file.name)\n",
        "\n",
        "        # Copy corresponding image\n",
        "        img_file = img_dir / f\"{label_file.stem}.jpg\"\n",
        "        if img_file.exists():\n",
        "            shutil.copy(img_file, sub_img_dir / img_file.name)\n",
        "\n",
        "    return n_keep, len(all_labels)\n",
        "\n",
        "\n",
        "if SUBSAMPLE_PERCENT < 99.999:\n",
        "    # Subsample both splits\n",
        "    train_kept, train_total = subsample_dataset('train2017', SUBSAMPLE_PERCENT)\n",
        "    val_kept, val_total = subsample_dataset('val2017', SUBSAMPLE_PERCENT)\n",
        "\n",
        "    print(f\"\\nâœ“ Subsampling complete:\")\n",
        "    print(f\"  Training: {train_total:,} â†’ {train_kept:,} images\")\n",
        "    print(f\"  Validation: {val_total:,} â†’ {val_kept:,} images\")\n",
        "else:\n",
        "    SUBSAMPLE_SUFFIX = \"\"\n",
        "    print(\"âœ“ No subsampling required\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_data_config"
      },
      "source": [
        "## Step 6: Setup COCO Data Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "data_config",
        "outputId": "2357a671-9401-4a02-a184-ab3400dcba11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Configuring Dataset Paths\n",
            "======================================================================\n",
            "âœ“ COCO configuration created\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"Configuring Dataset Paths\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# COCO data config (if not already correct)\n",
        "coco_yaml_content = f\"\"\"\n",
        "# COCO 2017 dataset\n",
        "\n",
        "path: {os.getcwd()}/coco\n",
        "train: images/train2017{SUBSAMPLE_SUFFIX}\n",
        "val: images/val2017{SUBSAMPLE_SUFFIX}\n",
        "\n",
        "nc: {len(CLASSES_TO_KEEP)}\n",
        "\n",
        "# Classes\n",
        "names:\n",
        "  0: person\n",
        "  1: bicycle\n",
        "  2: car\n",
        "  3: motorcycle\n",
        "  4: airplane\n",
        "  5: bus\n",
        "  6: train\n",
        "  7: truck\n",
        "  8: boat\n",
        "  9: bird\n",
        "  10: cat\n",
        "  11: dog\n",
        "  12: horse\n",
        "  13: sheep\n",
        "  14: cow\n",
        "  15: elephant\n",
        "  16: bear\n",
        "\"\"\"\n",
        "\n",
        "with open('data/coco.yaml', 'w') as f:\n",
        "    f.write(coco_yaml_content)\n",
        "\n",
        "print(\"âœ“ COCO configuration created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_config"
      },
      "source": [
        "## Step 7: Configure Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TgLuSYyQ25D"
      },
      "outputs": [],
      "source": [
        "#!rm -rf /content/yolov9/runs/train/yolov9-s-edgetpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "config_training",
        "outputId": "082c7115-69da-4947-f2e8-0ea429054d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Training Configuration\n",
            "======================================================================\n",
            "\n",
            "ðŸ“‹ Training Configuration:\n",
            "   Model: YOLOv9-s with ReLU6 activation\n",
            "   Starting weights: yolov9-s-converted.pt\n",
            "   Batch size: 24\n",
            "   Image size: 640x640\n",
            "   Epochs: 10\n",
            "   Checkpoint frequency: every 3 epochs\n",
            "\n",
            "ðŸ’¡ Colab Free Tier Tips:\n",
            "   â€¢ Training will take ~3-5 hours\n",
            "   â€¢ Session may disconnect - rerun next cell to resume\n",
            "   â€¢ Don't close the browser tab\n",
            "   â€¢ Checkpoints saved automatically\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"Training Configuration\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 24       # Adjust if out of memory (try 8 or 12 or 16)\n",
        "EPOCHS = 20           # Fine-tuning epochs\n",
        "IMAGE_SIZE = 640      # Match pre-training size\n",
        "CHECKPOINT_FREQ = 3   # Save every N epochs\n",
        "\n",
        "print(f\"\\nðŸ“‹ Training Configuration:\")\n",
        "print(f\"   Model: YOLOv9-s with ReLU6 activation\")\n",
        "print(f\"   Starting weights: {weights_file}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Checkpoint frequency: every {CHECKPOINT_FREQ} epochs\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ Colab Free Tier Tips:\")\n",
        "print(f\"   â€¢ Training will take ~3-5 hours\")\n",
        "print(f\"   â€¢ Session may disconnect - rerun next cell to resume\")\n",
        "print(f\"   â€¢ Don't close the browser tab\")\n",
        "print(f\"   â€¢ Checkpoints saved automatically\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "start_training"
      },
      "source": [
        "## Step 8: Start Training\n",
        "\n",
        "Leave browser tab open while this runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCFldBD8yGNL",
        "outputId": "92008548-f5e2-4615-a96e-b9e838587b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patching train_dual.py for PyTorch 2.8 compatibility...\n",
            "âœ“ train_dual.py patched successfully\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# FIX: Patch train_dual.py to allow loading pretrained weights\n",
        "# ============================================================================\n",
        "print(\"Patching train_dual.py for PyTorch 2.8 compatibility...\")\n",
        "\n",
        "# Read the file\n",
        "with open('train_dual.py', 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Find and replace the torch.load line\n",
        "old_line = \"ckpt = torch.load(weights, map_location='cpu')  # load checkpoint to CPU to avoid CUDA memory leak\"\n",
        "new_line = \"ckpt = torch.load(weights, map_location='cpu', weights_only=False)  # load checkpoint to CPU to avoid CUDA memory leak\"\n",
        "\n",
        "if old_line in content:\n",
        "    content = content.replace(old_line, new_line)\n",
        "\n",
        "    # Write back\n",
        "    with open('train_dual.py', 'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "    print(\"âœ“ train_dual.py patched successfully\")\n",
        "else:\n",
        "    print(\"âš ï¸  Line not found - manual edit needed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "train",
        "outputId": "8196025a-f194-4b88-94ce-8bfdf72e13d1",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STARTING TRAINING\n",
            "======================================================================\n",
            "You can minimize browser but don't close the tab\n",
            "\n",
            "======================================================================\n",
            "\n",
            "2025-11-10 12:09:02.588121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762776542.847725    5421 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762776542.920591    5421 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762776543.458684    5421 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762776543.458722    5421 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762776543.458725    5421 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762776543.458731    5421 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-10 12:09:03.508261: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mtrain_dual: \u001b[0mweights=yolov9-s-converted.pt, cfg=models/detect/yolov9-s-relu6.yaml, data=data/coco.yaml, hyp=data/hyps/hyp.scratch-high.yaml, epochs=10, batch_size=24, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=0, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=4, project=runs/train, name=yolov9-s-relu6, exist_ok=True, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=3, seed=0, local_rank=-1, min_items=0, close_mosaic=0, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "YOLO ðŸš€ v0.1-104-g5b1ea9a Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, obj=0.7, obj_pw=1.0, dfl=1.5, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO ðŸš€ in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO ðŸš€ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 42.4MB/s]\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "\u001b[34m\u001b[1mactivation:\u001b[0m nn.ReLU6()\n",
            "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 2]                 \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     31104  models.common.ELAN1                     [64, 64, 64, 32]              \n",
            "  3                -1  1     73984  models.common.AConv                     [64, 128]                     \n",
            "  4                -1  1    258432  models.common.RepNCSPELAN4              [128, 128, 128, 64, 3]        \n",
            "  5                -1  1    221568  models.common.AConv                     [128, 192]                    \n",
            "  6                -1  1    579648  models.common.RepNCSPELAN4              [192, 192, 192, 96, 3]        \n",
            "  7                -1  1    442880  models.common.AConv                     [192, 256]                    \n",
            "  8                -1  1   1028864  models.common.RepNCSPELAN4              [256, 256, 256, 128, 3]       \n",
            "  9                -1  1    164608  models.common.SPPELAN                   [256, 256, 128]               \n",
            " 10                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 11           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 12                -1  1    628800  models.common.RepNCSPELAN4              [448, 192, 192, 96, 3]        \n",
            " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 14           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 15                -1  1    283008  models.common.RepNCSPELAN4              [320, 128, 128, 64, 3]        \n",
            " 16                -1  1    110784  models.common.AConv                     [128, 96]                     \n",
            " 17          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
            " 18                -1  1    598080  models.common.RepNCSPELAN4              [288, 192, 192, 96, 3]        \n",
            " 19                -1  1    221440  models.common.AConv                     [192, 128]                    \n",
            " 20           [-1, 9]  1         0  models.common.Concat                    [1]                           \n",
            " 21                -1  1   1061632  models.common.RepNCSPELAN4              [384, 256, 256, 128, 3]       \n",
            " 22                 8  1    164608  models.common.SPPELAN                   [256, 256, 128]               \n",
            " 23                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 24           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 25                -1  1    628800  models.common.RepNCSPELAN4              [448, 192, 192, 96, 3]        \n",
            " 26                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 27           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 28                -1  1    283008  models.common.RepNCSPELAN4              [320, 128, 128, 64, 3]        \n",
            " 29[28, 25, 22, 15, 18, 21]  1   2955014  models.yolo.DualDDetectEdgeTPU          [17, [128, 192, 256, 128, 192, 256]]\n",
            "yolov9-s-relu6 summary: 1219 layers, 9755750 parameters, 9755718 gradients, 39.7 GFLOPs\n",
            "\n",
            "Transferred 1254/1772 items from yolov9-s-converted.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 291 weight(decay=0.0), 305 weight(decay=0.0005625000000000001), 303 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0m1 validation error for InitSchema\n",
            "size\n",
            "  Field required [type=missing, input_value={'scale': (0.8, 1.0), 'ra...: None, 'strict': False}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolov9/coco/labels/train2017_sub... 8614 images, 0 backgrounds, 0 corrupt: 100% 8614/8614 [00:01<00:00, 8128.97it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolov9/coco/labels/train2017_sub.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (7.0GB ram): 100% 8614/8614 [00:06<00:00, 1362.93it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolov9/coco/labels/val2017_sub... 364 images, 0 backgrounds, 0 corrupt: 100% 364/364 [00:00<00:00, 2436.87it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolov9/coco/labels/val2017_sub.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.3GB ram): 100% 364/364 [00:00<00:00, 1244.03it/s]\n",
            "Plotting labels to runs/train/yolov9-s-relu6/labels.jpg... \n",
            "/content/yolov9/train_dual.py:255: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/yolov9-s-relu6\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "  0% 0/359 [00:00<?, ?it/s]/content/yolov9/train_dual.py:313: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(amp):\n",
            "        0/9      9.94G      4.464      6.559      5.351        203        640:   0% 0/359 [00:04<?, ?it/s]Exception in thread Thread-31 (plot_images):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 300, in plot_images\n",
            "    annotator.box_label(box, label, color=color)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 86, in box_label\n",
            "    w, h = self.font.getsize(label)  # text width, height\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
            "WARNING âš ï¸ TensorBoard graph visualization failure Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions\n",
            "        0/9      9.94G      4.464      6.559      5.351        203        640:   0% 1/359 [00:06<41:26,  6.94s/it]/content/yolov9/train_dual.py:313: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(amp):\n",
            "        0/9      10.2G      4.558      6.469      5.347        266        640:   1% 2/359 [00:07<20:01,  3.37s/it]Exception in thread Thread-32 (plot_images):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 300, in plot_images\n",
            "    annotator.box_label(box, label, color=color)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 86, in box_label\n",
            "    w, h = self.font.getsize(label)  # text width, height\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
            "        0/9      10.7G      4.619      6.399      5.346        234        640:   1% 3/359 [00:08<13:04,  2.20s/it]Exception in thread Thread-33 (plot_images):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 300, in plot_images\n",
            "    annotator.box_label(box, label, color=color)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 86, in box_label\n",
            "    w, h = self.font.getsize(label)  # text width, height\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
            "        0/9      13.5G      4.168      4.944      4.797        203        640: 100% 359/359 [05:03<00:00,  1.18it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:05<00:00,  1.46it/s]\n",
            "                   all        364       1621      0.921     0.0423     0.0627     0.0293\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/9      14.7G      2.763      3.121      3.369        222        640: 100% 359/359 [04:50<00:00,  1.24it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:04<00:00,  1.70it/s]\n",
            "                   all        364       1621      0.407      0.267      0.258      0.142\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/9      14.7G      2.236      2.479      2.585        185        640: 100% 359/359 [04:48<00:00,  1.24it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:04<00:00,  1.71it/s]\n",
            "                   all        364       1621      0.523      0.493      0.485      0.312\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/9      14.7G      2.017      2.167      2.254        203        640: 100% 359/359 [04:45<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:04<00:00,  1.69it/s]\n",
            "                   all        364       1621      0.623      0.532      0.576      0.381\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        4/9      14.7G      1.905      2.005      2.106        187        640: 100% 359/359 [04:45<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:04<00:00,  1.72it/s]\n",
            "                   all        364       1621      0.682      0.509      0.574       0.39\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        5/9      14.7G      1.826      1.889      2.024        211        640: 100% 359/359 [04:45<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:04<00:00,  1.71it/s]\n",
            "                   all        364       1621      0.607      0.532      0.566      0.378\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        6/9      14.7G       1.77      1.815      1.968        188        640: 100% 359/359 [04:44<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:04<00:00,  1.71it/s]\n",
            "                   all        364       1621      0.729      0.577      0.665      0.467\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        7/9      14.7G      1.748      1.752      1.935        245        640: 100% 359/359 [04:46<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:04<00:00,  1.70it/s]\n",
            "                   all        364       1621      0.668      0.615      0.678      0.486\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        8/9      14.7G      1.702      1.675      1.898        274        640: 100% 359/359 [04:45<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:04<00:00,  1.71it/s]\n",
            "                   all        364       1621      0.724      0.569      0.669      0.479\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        9/9      14.7G      1.663      1.628      1.864        289        640: 100% 359/359 [04:44<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 8/8 [00:04<00:00,  1.71it/s]\n",
            "                   all        364       1621       0.72      0.606       0.69      0.499\n",
            "\n",
            "10 epochs completed in 0.819 hours.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov9/train_dual.py\", line 644, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov9/train_dual.py\", line 538, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/content/yolov9/train_dual.py\", line 413, in train\n",
            "    strip_optimizer(f)  # strip optimizers\n",
            "    ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/yolov9/utils/general.py\", line 999, in strip_optimizer\n",
            "    x = torch.load(f, map_location=torch.device('cpu'))\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1529, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray._reconstruct])` or the `torch.serialization.safe_globals([numpy._core.multiarray._reconstruct])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
            "\n",
            "======================================================================\n",
            "âœ“ TRAINING COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*70)\n",
        "print(\"You can minimize browser but don't close the tab\\n\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# Run training\n",
        "!python train_dual.py \\\n",
        "  --workers 4 \\\n",
        "  --device 0 \\\n",
        "  --batch-size {BATCH_SIZE} \\\n",
        "  --data data/coco.yaml \\\n",
        "  --img {IMAGE_SIZE} \\\n",
        "  --cfg models/detect/yolov9-s-relu6.yaml \\\n",
        "  --weights {weights_file} \\\n",
        "  --name yolov9-s-relu6 \\\n",
        "  --hyp data/hyps/hyp.scratch-high.yaml \\\n",
        "  --epochs {EPOCHS} \\\n",
        "  --save-period {CHECKPOINT_FREQ} \\\n",
        "  --cache \\\n",
        "  --exist-ok\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ“ TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download"
      },
      "source": [
        "## Step 9: Download Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_model",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "0830fe0a-99a8-4917-b020-f3db19f1834e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Download Trained Model\n",
            "======================================================================\n",
            "âœ“ Model ready: /content/yolov9-s-relu6-best.pt\n",
            "  File size: 79.88 MB\n",
            "\n",
            "Downloading to your computer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_78f70657-8b62-4119-bff7-460ff6495d73\", \"yolov9-s-relu6-best.pt\", 79878777)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Download complete!\n",
            "âœ“ Model ready: /content/yolov9-s-relu6-last.pt\n",
            "  File size: 79.88 MB\n",
            "\n",
            "Downloading to your computer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d27f3ab9-2747-4643-83f6-90341961ff7e\", \"yolov9-s-relu6-last.pt\", 79878777)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Download complete!\n",
            "âœ“ Epoch summary ready: /content/epoch-results.csv\n",
            "  File size: 0.00 MB\n",
            "\n",
            "Downloading to your computer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2d70d0be-02c0-412c-8449-42ab400ba56f\", \"epoch-results.csv\", 1470)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Download complete!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Download Trained Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "best_weights = Path('runs/train/yolov9-s-relu6/weights/best.pt')\n",
        "last_weights = Path('runs/train/yolov9-s-relu6/weights/last.pt')\n",
        "epoch_summary = Path('runs/train/yolov9-s-relu6/results.csv')\n",
        "\n",
        "if best_weights.exists():\n",
        "    output_path = '/content/yolov9-s-relu6-best.pt'\n",
        "    shutil.copy(best_weights, output_path)\n",
        "    print(f\"âœ“ Model ready: {output_path}\")\n",
        "    print(f\"  File size: {Path(output_path).stat().st_size / 1e6:.2f} MB\")\n",
        "    print(\"\\nDownloading to your computer...\")\n",
        "    files.download(output_path)\n",
        "    print(\"\\nâœ“ Download complete!\")\n",
        "if last_weights.exists():\n",
        "    output_path = '/content/yolov9-s-relu6-last.pt'\n",
        "    shutil.copy(last_weights, output_path)\n",
        "    print(f\"âœ“ Model ready: {output_path}\")\n",
        "    print(f\"  File size: {Path(output_path).stat().st_size / 1e6:.2f} MB\")\n",
        "    print(\"\\nDownloading to your computer...\")\n",
        "    files.download(output_path)\n",
        "    print(\"\\nâœ“ Download complete!\")\n",
        "if epoch_summary.exists():\n",
        "    output_path = '/content/epoch-results.csv'\n",
        "    shutil.copy(epoch_summary, output_path)\n",
        "    print(f\"âœ“ Epoch summary ready: {output_path}\")\n",
        "    print(f\"  File size: {Path(output_path).stat().st_size / 1e6:.2f} MB\")\n",
        "    print(\"\\nDownloading to your computer...\")\n",
        "    files.download(output_path)\n",
        "    print(\"\\nâœ“ Download complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate"
      },
      "source": [
        "## Step 10: (Optional) Validate Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validation"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"Validating Trained Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "best_weights = Path('runs/train/yolov9-s-relu6/weights/best.pt')\n",
        "\n",
        "if best_weights.exists():\n",
        "    print(f\"âœ“ Best weights found: {best_weights}\\n\")\n",
        "\n",
        "    # Run validation\n",
        "    print(\"Running validation on COCO val set...\\n\")\n",
        "    !python val_dual.py \\\n",
        "      --data data/coco.yaml \\\n",
        "      --img {IMAGE_SIZE} \\\n",
        "      --batch 32 \\\n",
        "      --conf 0.001 \\\n",
        "      --iou 0.7 \\\n",
        "      --device 0 \\\n",
        "      --weights {best_weights} \\\n",
        "      --task val\n",
        "\n",
        "    print(\"\\nâœ“ Validation complete!\")\n",
        "else:\n",
        "    print(\"âš ï¸  Best weights not found. Training may have failed.\")\n",
        "    print(\"    Check training output above for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plot_results"
      },
      "source": [
        "## Step 11: (Optional) Plot Training Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Training Results\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results_file = Path('runs/train/yolov9-s-relu6/results.txt')\n",
        "\n",
        "if results_file.exists():\n",
        "    # Read results\n",
        "    results = pd.read_csv(results_file, sep=r'\\s+', header=0)\n",
        "\n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    fig.suptitle('YOLOv9-s ReLU Training Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Box loss\n",
        "    axes[0, 0].plot(results['epoch'], results['train/box_loss'], label='Train', linewidth=2)\n",
        "    axes[0, 0].plot(results['epoch'], results['val/box_loss'], label='Val', linewidth=2)\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Box Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Object loss\n",
        "    axes[0, 1].plot(results['epoch'], results['train/obj_loss'], label='Train', linewidth=2)\n",
        "    axes[0, 1].plot(results['epoch'], results['val/obj_loss'], label='Val', linewidth=2)\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].set_title('Object Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Class loss\n",
        "    axes[1, 0].plot(results['epoch'], results['train/cls_loss'], label='Train', linewidth=2)\n",
        "    axes[1, 0].plot(results['epoch'], results['val/cls_loss'], label='Val', linewidth=2)\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Loss')\n",
        "    axes[1, 0].set_title('Class Loss')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # mAP metrics\n",
        "    axes[1, 1].plot(results['epoch'], results['metrics/mAP50(B)'], label='mAP@0.5', linewidth=2)\n",
        "    axes[1, 1].plot(results['epoch'], results['metrics/mAP50-95(B)'], label='mAP@0.5:0.95', linewidth=2)\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('mAP')\n",
        "    axes[1, 1].set_title('Mean Average Precision')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_results.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Show final metrics\n",
        "    last_row = results.iloc[-1]\n",
        "    print(f\"\\nðŸ“Š Final Metrics (Epoch {int(last_row['epoch'])}):\")\n",
        "    print(f\"   mAP@0.5: {last_row['metrics/mAP50(B)']:.4f}\")\n",
        "    print(f\"   mAP@0.5:0.95: {last_row['metrics/mAP50-95(B)']:.4f}\")\n",
        "else:\n",
        "    print(\"âš ï¸  Results file not found\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}